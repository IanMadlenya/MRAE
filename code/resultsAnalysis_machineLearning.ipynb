{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results analysis: machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bokeh.charts import Bar\n",
    "from bokeh.io import output_notebook, show, output_file, reset_output, gridplot, save\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import HoverTool, ColumnDataSource,Range1d, LinearAxis\n",
    "from bokeh.palettes import YlOrRd7, Spectral7, RdBu10, PuBu9, RdYlGn10, OrRd9, YlGn9\n",
    "from bokeh.models import Span\n",
    "from datetime import datetime\n",
    "from pandas.tseries.offsets import *\n",
    "from scipy import stats\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import resultsAnalysis_utils as uti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from resultsAnalysis_dataLoading import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML results loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlResultsFilePath = '../results/dae/neuralNetwork/performances_ML.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlResults = pd.read_csv(filepath_or_buffer=mlResultsFilePath,\n",
    "                        sep=';',\n",
    "                        header=0)\n",
    "\n",
    "for i, param in enumerate(parametersSetML):\n",
    "    concatParams = mlResults['concatParams']\n",
    "    mlResults[param] = concatParams.str.split('_').str[i]\n",
    "mlResults = mlResults.replace('None', -1)\n",
    "for col in numericML: mlResults[[col]] = mlResults[[col]].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete these columns because it's the same for every record\n",
    "del mlResults['activFirstLayer']\n",
    "del mlResults['activSecondLayer']\n",
    "del mlResults['loss_func']\n",
    "del mlResults['optimizer']\n",
    "del mlResults['scalingFactor']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlResults['loss'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlResults['smoothness'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model evaluation\n",
    "- Sort models according to training loss and smoothness.\n",
    "- Compare the 2 measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedLoss = uti.rawSort(results=mlResults, metricToSortBy='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sortedLoss[['loss', 'smoothness', 'encoding_dim', 'denoising', 'dropoutProba', 'nb_epoch', 'batch_size']].head(51).to_latex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data compression seems to work better: less overfitted solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uti.displayMetricEvolution(data=sortedLoss, metric='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedSmoothness = uti.rawSort(results=mlResults, metricToSortBy='smoothness')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sortedSmoothness.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same comment as above according to smoothness criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uti.displayMetricEvolution(data=sortedSmoothness, metric='smoothness')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare loss and smoothness\n",
    "Let's compare the two metrics and see how configs are ranked according to loss/smoothness. Colored squares stand for a configuration.\n",
    "- Configs that lie on diagonal mean loss/smoothness are ranked the same\n",
    "- Configs that lie close to the diagonal mean they are almost ranked the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uti.xyComparison(sortedSmoothness['concatParams'], sortedLoss['concatParams'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Influence of parameters\n",
    "\n",
    "See influence of each parameter ceteris paribus.\n",
    "- encoding_dim\n",
    "- denoising\n",
    "- dropoutProba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Encoding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = mlResults.groupby(['batch_size', 'nb_epoch', 'denoising', 'dropoutProba'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "uti.displayParameterEvolution(groupedData=grouped, metrics=['loss', 'smoothness'], parameter='encoding_dim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding data compression, results are in line with common sense: the more compressed, the higher the error/smoothness.\n",
    "\n",
    "However, regarding data augmentation, we may infer that globally the network learns something close to identity as long as the dropout proba is lower than 0.3, since no matter the encoding dimension, error and smoothness are roughly the same. We may want to choose a network with higher variance, such as a network with dropout proba >= 0.4. In that case, the error does not decrease necessarly with the encoding dimension as one may expect. May be due to high level of randomness and noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered = mlResults[mlResults['dropoutProba'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = filtered.groupby(['batch_size', 'nb_epoch', 'denoising', 'encoding_dim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "uti.displayParameterEvolution(groupedData=grouped, metrics=['loss', 'smoothness'], parameter='dropoutProba')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results in line with common sense: the noisier the input, the higher the error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot residuals\n",
    "\n",
    "Epsilon distribution of training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reset_output()\n",
    "output_notebook()\n",
    "output_file('../results/dae/neuralNetwork/resultsAnalysis/residuals.html')\n",
    "\n",
    "stocks = epsilon_test.columns.values\n",
    "\n",
    "res_test = epsilon_test.reset_index()\n",
    "res_train = epsilon_train.reset_index()\n",
    "\n",
    "grid = []\n",
    "subGrid = []\n",
    "for i, stock in enumerate(sorted(stocks)):\n",
    "    if i % 3 == 0 and i != 0:\n",
    "        grid.append(subGrid)\n",
    "        subGrid = []\n",
    "    p1 = figure(background_fill_color=\"#E8DDCB\",plot_width=500, plot_height=500)\n",
    "    p1.xaxis.axis_label_text_font_size = \"12pt\"\n",
    "    p1.legend.location = \"top_left\"\n",
    "    p1.xaxis.axis_label = 'r - r_hat'\n",
    "    p1.yaxis.visible = None\n",
    "    hist, edges = np.histogram(res_train[stock], density=True, bins=25)\n",
    "    p1.quad(top=hist,\n",
    "            bottom=0,\n",
    "            left=edges[:-1],\n",
    "            right=edges[1:],\n",
    "            fill_color=\"red\",\n",
    "            fill_alpha=0.5,\n",
    "            legend='train')\n",
    "    subGrid.append(p1)\n",
    "    hist, edges = np.histogram(res_test[stock], density=True, bins=25)\n",
    "    p1.quad(top=hist,\n",
    "            bottom=0,\n",
    "            left=edges[:-1],\n",
    "            right=edges[1:],\n",
    "            fill_color=\"green\",\n",
    "            fill_alpha=0.5,\n",
    "            legend='test')\n",
    "p = gridplot(grid)\n",
    "save(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test homogeneity\n",
    "\n",
    "Between test and train residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tests=[]\n",
    "for stock in epsilon_test.columns.values:\n",
    "    resTest = stats.ks_2samp(epsilon_test[stock], epsilon_train[stock])\n",
    "    tests.append([stock,\n",
    "                  round(resTest.statistic,3),\n",
    "                  round(resTest.pvalue,3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(tests).set_index(0).sort_values(by=2)[-10:].to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tests = []\n",
    "for stock in epsilon_test.columns.values:\n",
    "    testt = stats.ks_2samp(epsilon_test[stock], epsilon_train[stock])\n",
    "    if testt.pvalue < 0.05:\n",
    "        tests.append(1)\n",
    "        print('%s : %f')%(stock, testt.pvalue)\n",
    "string = '%f des résidus test/train ne sont statistiquements pas issus de la même distribution'\n",
    "print(string)%(len(tests) * 1. / len(epsilon_test.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure residuals for which H0 has been rejected matches a decent distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residuals and returns volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation between residuals and volatility\n",
    "Compute epsilon on training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = '10_100_False_None_adadelta_mse_200_10_tanh_linear'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_test = pd.read_csv(filepath_or_buffer='../results/dae/neuralNetwork/predictions/test_' + conf + '.csv',\n",
    "                               sep=';',\n",
    "                               header=0,\n",
    "                               index_col='Date',\n",
    "                               parse_dates=True)\n",
    "predictions_train = pd.read_csv(filepath_or_buffer='../results/dae/neuralNetwork/predictions/train_' + conf + '.csv',\n",
    "                               sep=';',\n",
    "                               header=0,\n",
    "                               index_col='Date',\n",
    "                               parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon_test = test - predictions_test\n",
    "epsilon_train = train - predictions_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute first sigma_return for training period. Start in 2005 so vol can be estimated over the past year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeVols(df):\n",
    "    vols = {}\n",
    "    for day in df.index.values:\n",
    "        if pd.to_datetime(str(day)) >= datetime(2005,1,1):\n",
    "            start = pd.to_datetime(str(day)) - DateOffset(months=12)\n",
    "            dfRescaled = df[start:day]\n",
    "            dailyVols = dfRescaled.std()\n",
    "            vols[day] = dailyVols\n",
    "\n",
    "    return pd.DataFrame.from_dict(vols, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vols_returns = computeVols(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use absolute values to stay in the same universe as volatility (which is positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon_train_abs = epsilon_train.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon_train_abs.loc['2005':].corrwith(vols_returns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Correlation between standard deviation of residuals and volatily\n",
    "\n",
    "Compute sigma_epsilon based on the same method as for the volatility above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vols_epsilon = computeVols(epsilon_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vols_epsilon.corrwith(vols_returns).quantile(0.05)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
